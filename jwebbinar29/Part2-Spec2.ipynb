{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a222e14-e1b1-4c20-abdb-859816f8d123",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/STScI-MIRI/MRS-ExampleNB/raw/main/assets/banner1.png' alt=\"stsci_logo\" width=\"1000px\"/> \n",
    "\n",
    "# NIRSpec BOTS JWebbinar Notebook 2: Spec 2 \n",
    "-----\n",
    "\n",
    "**Author**: Nikolay Nikolov, AURA Associate Scientist, NIRSpec branch\n",
    "<br>\n",
    "**Pipeline Version**: 1.12.0 on November 8, 2023\n",
    "<br>\n",
    "**Last Updated**: version 1.12.5 on December 8, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde75c01-3aa1-45ac-a207-584489a3e7f3",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Introduction](#intro)<br>\n",
    "   1.1. [Purpose of this Notebook](#purpose)<br>\n",
    "   1.2. [Input Data](#inputs)<br>\n",
    "2. [Setup](#setup)<br>\n",
    "   2.1. [Python imports and environment variables](#imports)<br>\n",
    "   2.2. [Data download and directory organization](#data)<br>\n",
    "3. [A look at the input data - the `rateints` files](#look)<br>\n",
    "4. [The JWST Stage 2 Pipeline `Spec2` and our custom steps](#spec2)<br>\n",
    "   4.1. [Running the `Assign_wcs step` of the pipeline and obtaining the wavelength map](#awcs)<br>\n",
    "        [Step 1: Obtain the wavelength map](#map)<br>\n",
    "        [Step 2: Organize and check the data. Perform 2d spectral cutouts ](#cutouts)<br>\n",
    "        [Step 3: Remove 1/f noise from the 2dcutouts](#1overf)<br>\n",
    "        [Step 4: Identify and replace bad pixels](#bad)<br>\n",
    "        [Step 5: Obtain centroid information using each 2D spectrum](#centroid)<br>\n",
    "        [Step 6: Spectral tracing](#tracing)<br>\n",
    "        [Step 7: Extract the spectra and obtain wavelength solution](#extraction)<br>\n",
    "        [Step 8: Produce white and spectroscopic light curves and fitting data](#lcs)<br>\n",
    "5. [Concluding remarks](#remarks)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac23e6-2b50-4df7-babd-49c9e6c3ef3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1.<font color='white'>-</font>Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b6533f-6ac9-4872-a292-f15ddfcb75e0",
   "metadata": {},
   "source": [
    "### 1.1.<font color='white'>-</font>Purpose of this Notebook<a class=\"anchor\" id=\"purpose\"></a> ###\n",
    "\n",
    "In this notebook we provide an example to get started on the exploration and production of NIRSpec BOTS light curves. In particular, we focus on outputs from `Spec2` stage of the pipeline, and go through our own spectral centroiding, tracing and extraction procedures. The goal of this notebook is to produce white and spectroscopic lightcurves along with fitting products, which JWST NIRSpec BOTS users might want to perform on their datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d9601-55ba-4f5e-80d0-5da85ac7faaf",
   "metadata": {},
   "source": [
    "### 1.2<font color='white'>-</font>Input Data<a class=\"anchor\" id=\"inputs\"></a> ###\n",
    "\n",
    "The input data for this notebook, a transit of WASP-39b observed in the BOTS G395H grism, are the `Detector1` `rateints` files. This full dataset can be found at https://stsci.box.com/s/r9k17tb2ig5vaivw6tvp52bupivo5wjv. The data set belongs to the <a href=\"https://www.stsci.edu/jwst/science-execution/approved-programs/dd-ers/program-1366\">JWST Early Release Science program ERS-1366,</a> observation 3, and includes 456 integrations of 70 groups with the SUB2048 subarray, covering a total of 10.56 hours.\n",
    "\n",
    "This notebook can be executed in two separate modes: 'process' and 'load'. Chosing the 'process' mode, the user can execute step-by-step and wait for the step to complete. Products created by the notebook will be saved. Selecting the 'load' mode would skip execution of the steps that take several tens of minutes to complete. Instead, the notebook will load pre-processed data. Finally, the notebook is designed to be run on both NRS1 and NRS2 data. The user needs to specify which detector wishes to analyze and execute all steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dac4ed-f24a-4ac5-8b72-757024a1ac28",
   "metadata": {},
   "source": [
    "2.<font color='white'>-</font>Setup <a class=\"anchor\" id=\"setup\"></a>\n",
    "------------------\n",
    "\n",
    "### 2.1.<font color='white'>-</font>Python imports and environment variables<a class=\"anchor\" id=\"imports\"></a> ###\n",
    "\n",
    "In this section we set a number of necessary things in order for the pipeline to run successfully.\n",
    "\n",
    "First we'll set the CRDS context; this dictates the versions of various pipeline reference files to use. \n",
    "\n",
    "Next we'll import the various python packages that we're actually going to use in this notebook, including both generic utility functions and the actual pipeline modules themselves. Make sure to install the jwst pipeline, astropy, opencv-python and tqdm. \n",
    "\n",
    "Finally, we also define three directories: \n",
    "1. `working_folder` - where this notebook is placed, and where the remaining two directories will live.\n",
    "2. `spec2_input` - a directory that will contain the rateints (slope) files\n",
    "3. `spec2_results` - a directory with the pre-processed data from each step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7cd4ea-e796-4773-83e3-60e5caf1aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is needed when running from the Science Platform to pre-load some cache data\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "CRDS_PATH='/home/jovyan/crds_cache/'\n",
    "CRDS_SERVER_URL='https://jwst-crds.stsci.edu'\n",
    "\n",
    "# Copy the pre-loaded data into local writable folder\n",
    "working_folder = os.getcwd()+'/'\n",
    "spec2_results = working_folder + 'spec2_results/'\n",
    "print(spec2_results)\n",
    "if not os.path.exists(spec2_results):\n",
    "    os.makedirs(spec2_results)\n",
    "shutil.copytree('/home/shared/preloaded-fits/jwebbinar_29/JWebbinar2023-TSO/spec2_results/', spec2_results,dirs_exist_ok=True)\n",
    "\n",
    "# Set Paths\n",
    "spec2_input = '/home/shared/preloaded-fits/jwebbinar_29/JWebbinar2023-TSO/spec2_input/'\n",
    "spec2_results = working_folder + 'spec2_results/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69178b-2f1d-438d-963f-6cbe3e5ffbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to set these enviromental variables for this notebook to work properly:\n",
    "%set_env CRDS_PATH $CRDS_PATH\n",
    "%set_env CRDS_SERVER_URL $CRDS_SERVER_URL\n",
    "\n",
    "\n",
    "# General functions\n",
    "import os\n",
    "import time as tt\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "# Python math functions\n",
    "import numpy as np\n",
    "from numpy.polynomial import chebyshev\n",
    "from scipy.ndimage import median_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import interp1d, splrep,splev\n",
    "\n",
    "# Plot functions\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Rectangle\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Astropy and other spectroscopy utilities\n",
    "import astropy\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import astropy_mpl_style\n",
    "from astropy.io import fits, ascii\n",
    "from astropy.modeling import models, fitting\n",
    "from astropy.modeling.models import Gaussian1D\n",
    "from astropy.table import Table\n",
    "from astropy.time import Time\n",
    "\n",
    "from astropy.io import fits\n",
    "import astropy.io.fits as fits\n",
    "from astropy.timeseries import LombScargle\n",
    "from astropy.utils.data import download_file\n",
    "\n",
    "# JWST Pipeline\n",
    "from jwst import datamodels\n",
    "from jwst.pipeline import calwebb_spec2\n",
    "from jwst.assign_wcs import nirspec\n",
    "from jwst.assign_wcs import AssignWcsStep\n",
    "\n",
    "# Python utility functions task progress tracker\n",
    "from tqdm import tqdm\n",
    "\n",
    "import crds\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the folder on which we will be working on (here we'll store all our outputs):\n",
    "#working_folder = os.getcwd()+'/'\n",
    "#spec2_input = os.getcwd()+'/spec2_input/'\n",
    "#spec2_results = working_folder + 'spec2_results/'\n",
    "\n",
    "# if not os.path.exists(spec2_input):\n",
    "#     os.makedirs(spec2_input)\n",
    "    \n",
    "# if not os.path.exists(spec2_results):\n",
    "#     os.makedirs(spec2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2567bb77-0edc-4154-8db8-32dd91578a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(working_folder)\n",
    "print(spec2_input)\n",
    "print(spec2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f2064f-3ddc-476f-8c15-7fdc4aa0d6d2",
   "metadata": {},
   "source": [
    "### 2.2.<font color='white'>-</font>Data download and directory organization<a class=\"anchor\" id=\"data\"></a> ###\n",
    "\n",
    "This notebook will create 'spec2_input' and 'spec2_results' folders to store the input data and results from the individual steps detailed below. Folder 'spec2_input' contains the rateints data needed to start the notebook. Directory 'spec2_results' contains pre-processed data products, results from each step run at the creation of the notebook. The purpose of this directory is to provide the user with pre-processed data of the time consuming steps.\n",
    "\n",
    "**Note**: Please, download the needed data from https://stsci.box.com/s/r9k17tb2ig5vaivw6tvp52bupivo5wjv by clicking on the link. A new window will be loaded in your browser. Identify a tab with three dots (More Options), click on it and select download. The data size is ~1.7GB and takes ~5 minutes to download. Once downloaded, please enter the 'JWebbinar2023-TSO' directory and move the directories 'spec2_input' and 'spec2_results' one level up i.e., in the same directory, where this notebook is saved. The empty directory 'JWebbinar2023-TSO' can optionally be deleted. Alternatively, one can simply select the two folders for download individually from box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616fd975-c664-4175-ab75-7bdedc98e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "# Data and file configuration step\n",
    "#----------------------------------------\n",
    "\n",
    "# Choose a run mode with two options: \n",
    "#    process - processes the data (takes several tens of minutes)\n",
    "#    load - reads pre-processed data (all data will be downloaded if not present)\n",
    "run_mode = 'load' #\n",
    "process_detector = 'nrs1' # choose 'nrs1', 'nrs2'\n",
    "\n",
    "# Define the rateints files (aka slope or rate images)\n",
    "if process_detector == 'nrs1':\n",
    "    files = ['jw01366003001_04101_00001-seg001_nrs1_rateints.fits',\n",
    "             'jw01366003001_04101_00001-seg002_nrs1_rateints.fits',\n",
    "             'jw01366003001_04101_00001-seg003_nrs1_rateints.fits']\n",
    "\n",
    "if process_detector == 'nrs2':\n",
    "    files = ['jw01366003001_04101_00001-seg001_nrs2_rateints.fits', \n",
    "             'jw01366003001_04101_00001-seg002_nrs2_rateints.fits',\n",
    "             'jw01366003001_04101_00001-seg003_nrs2_rateints.fits']\n",
    "\n",
    "\n",
    "#----------------------------------------\n",
    "\n",
    "preprocessed_file_names = ['_step_01_wavelength_map.pickle',\n",
    "                           '_step_02_spectra2D_cutouts.pickle',\n",
    "                           '_step_03_spectra2D_1overf.pickle',\n",
    "                           '_step_04_nominal_psf_correction.pickle',\n",
    "                           '_step_05_centroiding.pickle',\n",
    "                           '_step_06_spectral_trace.pickle',\n",
    "                           '_step_07_spectra1D.pickle',\n",
    "                           '_step_08_light_curves.pickle']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa884dd-5396-4814-bff5-5c42a9bcf8fb",
   "metadata": {},
   "source": [
    "Extract data file root names. These are needed in the remaining of the notebook for data read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d430a9-2026-4578-bdea-b046c7f83cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "froot = [files[i].replace('rateints.fits', '') for i in range(len(files))]\n",
    "print(froot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed22bf5e-d090-4384-83f4-a5a6b105f08a",
   "metadata": {},
   "source": [
    "3.<font color='white'>-</font>A look at the input data - the `rateints` files<a class=\"anchor\" id=\"look\"></a>\n",
    "------------------\n",
    "\n",
    "The `rateints` files contain the slope images for each integration, which is the product we want to use for our BOTS analysis. Let's take a look at the contents of the first segment file using the JWST datamodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb8240-ba8e-4c6a-955c-9bad45b12a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_ints = datamodels.open(spec2_input + files[0])\n",
    "print(rate_ints)\n",
    "\n",
    "# Print the file inventory\n",
    "rate_ints.info()\n",
    "\n",
    "\n",
    "# Let's plot five integrations from the data cube and see how the data looks like\n",
    "\n",
    "# Define the range for the colorbar of the plot\n",
    "if process_detector == 'nrs1':\n",
    "    f_lo = -1 # lower limit for flux\n",
    "    f_hi = +1# upper limit for flux\n",
    "\n",
    "if process_detector == 'nrs2':\n",
    "    f_lo = -1 # lower limit for flux\n",
    "    f_hi = +1# upper limit for flux\n",
    "\n",
    "\n",
    "sl_fig, axs = plt.subplots(ncols=1, nrows=5, figsize=[18, 17])\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    im = ax.imshow(rate_ints.data[i, :, :], \n",
    "                   origin='lower', \n",
    "                   aspect='auto', \n",
    "                   interpolation='none',\n",
    "                   cmap='inferno', \n",
    "                   clim=(f_lo, f_hi))\n",
    "    if i == 2:\n",
    "        ax.set_ylabel('Y-row, pixel', fontsize=15)\n",
    "    ax.set_title('int = {0}'.format(i+1))\n",
    "\n",
    "ax.set_xlabel('X-column, pixel', fontsize=15)\n",
    "sl_fig.subplots_adjust(right=0.95)\n",
    "cbar_ax = sl_fig.add_axes([0.98, 0.2, 0.02, 0.6])\n",
    "cbar = sl_fig.colorbar(im, cax=cbar_ax)\n",
    "cbar.set_label('DN/s', fontsize='x-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8cca2-69c1-4d9c-b7ae-3fcc70c7687e",
   "metadata": {},
   "source": [
    "\n",
    "### <font color='white'>-</font>4. The JWST Stage 2 Pipeline `Spec2` and our custom steps<a class=\"anchor\" id=\"spec2\"></a>\n",
    "\n",
    "In Stage 2 of the pipeline, the following calibrations are applied to the NIRSpec BOTS data:\n",
    "\n",
    "1. Assigning a world coordinate system WCS for images and wavelength map for spectra -- WILL BE RUN\n",
    "3. Extracting 2d spectral ctouts -- WILL BE SKIPPED WITH OUR WORK AROUND\n",
    "4. Flat fielding -- WILL BE SKIPPED\n",
    "5. Assigning a source type -- WILL BE SKIPPED\n",
    "6. Photometric calibration -- WILL BE SKIPPED\n",
    "7. Pixel replace  -- WILL BE SKIPPED WITH OUR WORK AROUND\n",
    "8. Resampling spectrum -- WILL BE SKIPPED\n",
    "9. Spectral extraction  -- WILL BE SKIPPED WITH OUR WORK AROUND\n",
    "\n",
    "In what follows, we will be running the first step of the pipeline, and provide the necessary additional steps outside of the pipeline with the goal to produce white and spectroscopic light curves and complementary data for detrending light curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50eafa-84ae-4551-a8c8-160189d6c730",
   "metadata": {},
   "source": [
    "### <font color='white'>-</font>4.1. Running the `Assign_wcs step` of the pipeline and obtaining the wavelength map<a class=\"anchor\" id=\"awcs\"></a>\n",
    "\n",
    "\n",
    "The role of this step is to attach world coordinate system information to the data in a new extension called WCS. By doing so, every pixel in the detector array has an associated RA, Dec and wavelength. In addition, the step also defines a bounding box region over which these values are defined with NaNs everywhere else, owing to the current limitations of calibration data.\n",
    "\n",
    "**NOTE**. The assignment of wavelength values to pixels assumes that the target is placed at a specific location, i.e. the nominal pointing location for the employed subarray. The WATA acquisition procedure should be capable of placing the target there with an accuracy of < 10 mas, and an image will be taken through the WATA filter, which is accessible along with the data to verify the source positioning. For reference, 1 pixel corresponds to 0.1 arcsec for NIRSpec, or the accuracy of positioning the target on the detecor should be 0.1 pixel. \n",
    "\n",
    "Importantly, offsets of the target from its nominal position can cause small offset of the spectrum on the detector. Causes for TA offsets include inaccurate target coordinates, saturated pixels and signal to noise lower than ~20.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c56629-98a6-4051-9599-874a61a26cf8",
   "metadata": {},
   "source": [
    "The next cell runs this the awcs step for all files. Let's also measure how much time it takes to run on this computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2687d577-2039-4c00-afec-8caa86d84b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as tt\n",
    "from jwst.assign_wcs import AssignWcsStep\n",
    "\n",
    "\n",
    "start = tt.time()\n",
    "\n",
    "# There is no need to specify the detector identifier, because it is in the file names and the pipeline will use it from there.\n",
    "\n",
    "for i in range(len(files)):\n",
    "    AssignWcsStep.call(spec2_input + files[i], output_dir = spec2_results, save_results = True, skip = False)\n",
    "\n",
    "\n",
    "end = tt.time()\n",
    "print(\"Run time: \", round(end-start,1), \" sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7d079-ba0a-49a3-b877-f9fc5b8add2b",
   "metadata": {},
   "source": [
    "OK, the step ran nicely taking just 4 seconds, and saved the output in the `spec2_results` directory. We continue with our custom steps below, which we will call Steps 1 to 7. We will save the results from each step in a pickle file, and will plot results in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b8770d-c2ec-4a10-87c7-caadd1b1787c",
   "metadata": {},
   "source": [
    "# <font color='white'>-</font>Step 1: Obtain the wavelength map<a class=\"anchor\" id=\"map\"></a>\n",
    "\n",
    "Now let's load the awcs data inside a JWST datamodel and obtain the wavelength map. This step takes about 9min to run.\n",
    "\n",
    "It should be noted that the cell below refers only to NIRSpec data, and not data from all JWST instruments. The reason for that is that NIRSpec requires an extra step that specifies which slit should be used to obtain the wavelength map. This step is the following: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46639a13-5c85-4313-b2ee-1db82aa963e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwst import assign_wcs\n",
    "from gwcs import wcstools\n",
    "\n",
    "# Open the awcs step as JWST data model for the first segment\n",
    "awcs = datamodels.open(spec2_results + froot[0] + 'assignwcsstep.fits')\n",
    "\n",
    "# Specify the slit identifier using the metadata; it should be S1600A1 for BOTS\n",
    "print('Obtaining wavelength solution using slit:', awcs.meta.instrument.fixed_slit)\n",
    "wcs_out = assign_wcs.nrs_wcs_set_input(awcs, awcs.meta.instrument.fixed_slit)\n",
    "\n",
    "# Get the output wavelength map from the bounding box along with bounding box coordinates. Define bounding box depending on the \n",
    "# subarray size (by default, wcs_out maps the slit in the detector for NIRSpec)\n",
    "wcs_out.bounding_box = ( (-0.5, awcs.data.shape[2]-0.5), \n",
    "                         (-0.5, awcs.data.shape[1]-0.5) \n",
    "                       )   \n",
    "#print(wcs_out)\n",
    "\n",
    "# Obtain bounding box data\n",
    "bb_columns, bb_rows = wcstools.grid_from_bounding_box( wcs_out.bounding_box )\n",
    "_, _, bb_wavelength_map = wcs_out(bb_columns, bb_rows)\n",
    "\n",
    "# Prepare and fill wavelength map given this bounding box (fill the rest with nans because later nans are ommited):\n",
    "wavelength_map = np.full([ awcs.data.shape[1], awcs.data.shape[2] ], np.nan)\n",
    "\n",
    "# Obtain the wavelength map\n",
    "for i in range( bb_wavelength_map.shape[0] ):\n",
    "    for j in range( bb_wavelength_map.shape[1] ):\n",
    "            wavelength_map[int(bb_rows[i, j]), int(bb_columns[i, j])] = bb_wavelength_map[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447bd7a-6c43-4c33-8b22-bbf2733a355e",
   "metadata": {},
   "source": [
    "Pickle the wavelength solution, so we don't need to reobtain it every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72d93b-4acf-4e34-b843-992015a9f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wave_map = {'wavelength_map':wavelength_map}\n",
    "\n",
    "print('\\nSvaing ' + process_detector + preprocessed_file_names[0] + '.')\n",
    "pickle_out = open( spec2_results + '/' + process_detector + preprocessed_file_names[0], 'wb')\n",
    "pickle.dump(wave_map, pickle_out)\n",
    "pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced771e-c298-40dd-888f-46a5bb8ce585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the map\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(211)\n",
    "plt.title('Wavelength Map for ' + process_detector.upper(), fontsize=20)\n",
    "im1 = plt.imshow(wavelength_map, \n",
    "                 interpolation='None', \n",
    "                 aspect='auto', \n",
    "                 cmap='inferno', \n",
    "                 origin='lower')\n",
    "\n",
    "plt.xlabel('x-column', fontsize=15)\n",
    "plt.ylabel('y-row', fontsize=15)\n",
    "\n",
    "cb1 = plt.colorbar(label=r'Wavelength, [$\\mu$m]')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(len(wavelength_map[0,:]),len(wavelength_map[:,0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a05a2-feab-410f-bc7a-18790a98243f",
   "metadata": {},
   "source": [
    "The wavelength map on the plot covers the location, where the spectrum is imaged by the detector. The empty plot area has pixel values set to nans, owing to missing calibration data. We will come back to this wavelength map when we have the extracted one dimensional spectra and will use it to assign a wavelength data for the spectra. Let's continue with the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a31c976-581a-490c-b735-8ca1bddcf449",
   "metadata": {},
   "source": [
    "# <font color='white'>-</font>Step 2: Organize and check the data. Perform 2d spectral cutouts <a class=\"anchor\" id=\"cutouts\"></a>\n",
    "\n",
    "The goal of this step is to cut unnecessary pixels from the two-dimesional spectra. This saves computational time and trims data that will not be utilized. To perform the step, we would first load the science extensions for all integrations, covered by the three segments of the WASP-39b observation and put them in one big array. We will also do this for the error and data quality arrays, and time stamps using the slope images from the `AWCS` step. This would make handling the data easier throughout the notebook. Keep in mind that such approach has the potential to run in data memory issues for data sets with large number of integrations (e.g., bright sources with small number of groups), in case you decide to adapt this notebook. Tips for such cases: combine only a few segments, or analyze each segment separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0bb3fd-deb0-4d8a-b323-ba5cc40cb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_segments = len(froot)\n",
    "print('Number of segments: ', n_segments)\n",
    "\n",
    "for i in range(n_segments):\n",
    "    print('\\nConcatenating segment ', i+1, ' out of ', n_segments)\n",
    "    ramp_HDUL = datamodels.open(spec2_results + froot[i] + 'assignwcsstep.fits')\n",
    "    #ramp_HDUL.info() # Use this command if you wish to take a look at the data model contents\n",
    "    ramp_sci = ramp_HDUL.data\n",
    "    ramp_err = ramp_HDUL.err\n",
    "    ramp_DQs = ramp_HDUL.dq\n",
    "    times    = ramp_HDUL.int_times.int_mid_BJD_TDB\n",
    "    print('Number of time stamps per segment: ', len(times))\n",
    "\n",
    "    # Concatenate the data from each segment into the large arrays\n",
    "    if i == 0:\n",
    "        all_spec    = ramp_sci\n",
    "        all_err     = ramp_err\n",
    "        all_DQs     = ramp_DQs\n",
    "        all_times   = times\n",
    "    if i > 0:\n",
    "        all_spec    = np.concatenate((all_spec,    ramp_sci), axis = 0)\n",
    "        all_err     = np.concatenate((all_err,     ramp_err), axis = 0)\n",
    "        all_DQs     = np.concatenate((all_DQs,     ramp_DQs), axis = 0)\n",
    "        all_times   = np.concatenate((all_times,   times))\n",
    "    ramp_HDUL.close()\n",
    "\n",
    "print('\\nDimensions of the concatenated array: ')\n",
    "print('number of images: ', len(all_spec[:,0,0]))\n",
    "print('x-size: ', len(all_spec[0,0,:]))\n",
    "print('y-size: ', len(all_spec[0,:,0]))\n",
    "print('Number of time stamps: ', len(all_times))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dff966-45aa-4783-b900-752ab820144d",
   "metadata": {},
   "source": [
    "Let's check the time stamps for any issues. If there are no problems, the time stamps from each segment will align in one continuous line. In case of issues, we would see jumps, and/or differences in the slopes from each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02ba9c-9df3-4bda-baf5-73f4c538ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check times\n",
    "int_t = np.arange(len(all_times))\n",
    "\n",
    "from pylab import rcParams\n",
    "fig, ax = plt.subplots()\n",
    "#plt.rcParams.update({'font.sans-serif':'Helvetica'})\n",
    "\n",
    "rcParams['figure.figsize'] = [7, 5]           # Figure dimensions\n",
    "rcParams['image.aspect']   = 1                     # Aspect ratio\n",
    "\n",
    "ax.tick_params(direction='in', axis='both', which='both', top='on', bottom='on', right='on', left='on')\n",
    "sc = plt.scatter(int_t, all_times,  s=60, edgecolors=\"black\", facecolors='none', linewidth=0.1)\n",
    "\n",
    "ax.set_xlabel('Integration', fontsize=16)\n",
    "ax.set_ylabel('INT_TIMES, int_mid_BJD_TDB', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# If you need to save the plot, just uncomment the code below.\n",
    "#pp = PdfPages(results_dir + '/JDvInd.pdf')\n",
    "#plt.savefig(pp,format='pdf',bbox_inches='tight',pad_inches=0.05, dpi=250)\n",
    "#pp.close()\n",
    "#clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a8786-8117-4036-b2e0-a3a9576a418d",
   "metadata": {},
   "source": [
    "All looks good!\n",
    "Let's move to the 2d cutouts. First we will plot one spectrum and decide where do we want to cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92702836-0313-477e-86b1-6853d20cd116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a 2D spectrum\n",
    "if process_detector == 'nrs1':\n",
    "    int_id  = 20 # select an integration to show\n",
    "    f_lo    = -150 # lower limit for flux\n",
    "    f_hi    = +150# upper limit for flux\n",
    "    \n",
    "    # Define cutout\n",
    "    xcut_low  = 650\n",
    "    xcut_high = all_spec.shape[2] - 15\n",
    "    print(xcut_low, xcut_high)\n",
    "\n",
    "if process_detector == 'nrs2':\n",
    "    int_id  = 20 # select an integration to show\n",
    "    f_lo    = -150 # lower limit for flux\n",
    "    f_hi    = +150# upper limit for flux\n",
    "    \n",
    "    # Define cutout\n",
    "    xcut_low  = 4\n",
    "    xcut_high = all_spec.shape[2] - 25\n",
    "    print(xcut_low, xcut_high)\n",
    "\n",
    "\n",
    "image_for_inspection = all_spec[int_id,:,:]\n",
    "image_for_inspection[:,xcut_low+1]  = -999.\n",
    "image_for_inspection[:,xcut_high+1] = -999.\n",
    "image_for_inspection[:,xcut_high+2] = -999.\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(211)\n",
    "plt.title('Spectrum before 2Dcut', fontsize=20)\n",
    "im1 = plt.imshow(image_for_inspection, \n",
    "                 interpolation='None', \n",
    "                 aspect='auto', \n",
    "                 cmap='inferno', \n",
    "                 origin='lower',\n",
    "                 clim=(f_lo, f_hi))\n",
    "\n",
    "plt.xlabel('x-column', fontsize=15)\n",
    "plt.ylabel('y-row', fontsize=15)\n",
    "\n",
    "cb1 = plt.colorbar(label=r'Counts, DN/s')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e782ed-d233-4e00-ae11-1df0044b2625",
   "metadata": {},
   "source": [
    "Next, we need to just trim the unnecessary data and plot again to check how the cutouts look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09442457-921a-4d20-a07b-ec4e6dbb70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cutouts of the 2d spectra and wavelength map\n",
    "\n",
    "all_spec = all_spec[:,:,xcut_low:xcut_high]\n",
    "all_err  = all_err[:,:,xcut_low:xcut_high]\n",
    "all_DQs  = all_DQs[:,:,xcut_low:xcut_high]\n",
    "wavelength_map = wavelength_map[:,xcut_low:xcut_high]\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(211)\n",
    "plt.title('Spectrum after 2Dcut', fontsize=20)\n",
    "im1 = plt.imshow(all_spec[int_id,:,:], \n",
    "                 interpolation='None', \n",
    "                 aspect='auto', \n",
    "                 cmap='inferno', \n",
    "                 origin='lower',\n",
    "                 clim=(f_lo, f_hi))\n",
    "\n",
    "plt.xlabel('x-column', fontsize=15)\n",
    "plt.ylabel('y-row', fontsize=15)\n",
    "\n",
    "cb1 = plt.colorbar(label=r'Counts, DN/s')\n",
    "\n",
    "\n",
    "spec_xlen = len(all_spec[0,0,:])\n",
    "spec_ylen = len(all_spec[0,:,0])\n",
    "nint      = len(all_spec[:,0,0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a84de7-3ab4-45f4-b291-61dec36ecee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(211)\n",
    "plt.title('Wavelength Map for ' + process_detector.upper(), fontsize=20)\n",
    "im1 = plt.imshow(wavelength_map, \n",
    "                 interpolation='None', \n",
    "                 aspect='auto', \n",
    "                 cmap='inferno', \n",
    "                 origin='lower')\n",
    "\n",
    "plt.xlabel('x-column', fontsize=15)\n",
    "plt.ylabel('y-row', fontsize=15)\n",
    "\n",
    "cb1 = plt.colorbar(label=r'Wavelength, [$\\mu$m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21052117-c860-4538-92ba-c6d40f4de7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pickle the 2D cutouts, so we don't have to run this step every time\n",
    "if (run_mode == 'process'):\n",
    "    print('\\nSvaing ' + process_detector + preprocessed_file_names[1] + '.')\n",
    "    \n",
    "    spectra2D_cutouts = {\\\n",
    "    'all_spec':all_spec, \\\n",
    "    'all_err':all_err, \\\n",
    "    'all_DQs':all_DQs, \\\n",
    "    'all_times':all_times, \\\n",
    "    'nint':nint, \\\n",
    "    'spec_xlen':spec_xlen, \\\n",
    "    'spec_ylen':spec_ylen, \\\n",
    "    'wavelength_map': wavelength_map\n",
    "    }\n",
    "    \n",
    "    pickle_out = open( spec2_results + '/' + process_detector + preprocessed_file_names[1], 'wb')\n",
    "    pickle.dump(spectra2D_cutouts, pickle_out)\n",
    "    pickle_out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38404b63-6eba-4561-a5c4-12c7225381f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the pickle file with the 2D cutouts\n",
    "if (run_mode == 'load'):\n",
    "    pickle_in = open( spec2_results + process_detector + preprocessed_file_names[1], 'rb')\n",
    "    \n",
    "    spectra2D_cutouts = pickle.load( pickle_in )\n",
    "    pickle_in.close() # close the file\n",
    "    \n",
    "    print('Showing the pickled data:')\n",
    "    \n",
    "    cnt = 0\n",
    "    for item in spectra2D_cutouts:\n",
    "        print('The data ', cnt, ' is : ', item)\n",
    "        cnt += 1\n",
    "    \n",
    "    all_spec = spectra2D_cutouts['all_spec']\n",
    "    all_err = spectra2D_cutouts['all_err']\n",
    "    all_DQs = spectra2D_cutouts['all_DQs']\n",
    "    all_times = spectra2D_cutouts['all_times']\n",
    "    nint = spectra2D_cutouts['nint']\n",
    "    spec_xlen = spectra2D_cutouts['spec_xlen']\n",
    "    spec_ylen = spectra2D_cutouts['spec_ylen']\n",
    "    wavelength_map = spectra2D_cutouts['wavelength_map']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89927a5-7b42-40ef-ae08-001a91478c4a",
   "metadata": {},
   "source": [
    "# <font color='white'>-</font>Step 3: Remove 1/f noise from the 2dcutouts <a class=\"anchor\" id=\"1overf\"></a>\n",
    "\n",
    "\n",
    "JWST detector readout electronics (aka SIDECAR ASICs) generate significant 1/f noise during detector operations and signal digitization. When using NIRSpec BOTS, this 1/f noise appears as vertical banding that spans the entire width of the 2d spectral image, and varies from column to column. If not handled properly, the 1/f noise can introduce systematic errors and extra scatter in the light curves. For more information, please visit: <a href=\"https://jwst-docs.stsci.edu/methods-and-roadmaps/jwst-time-series-observations/jwst-time-series-observations-noise-sources#JWSTTimeSeriesObservationsNoiseSources-1/fnoise\">JWST Time-Series Observations Noise Sources.</a>\n",
    "\n",
    "One way to remove 1/f noise for the BOTS data can be done by measuring the median count level of a column using pixels outside of the Point Spread Function (PSF). This median flux is then subtracted from the entire column. The function below performs this task for a given image using backreound region above and below the spectrum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d77c3d-ec60-4809-b159-012ef64f0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_one_over_f(image, blow, bhigh, detector_id):\n",
    "    # Inputs:\n",
    "    # image - image to correct, 2D array\n",
    "    # blow - lower column pixels for the two regions, a tuple of two values \n",
    "    # bhigh - higher column pixels for the two regions, a tuple of two values\n",
    "\n",
    "    y_lo_1, y_lo_2 = blow\n",
    "    y_hi_1, y_hi_2 = bhigh\n",
    "    \n",
    "    corrected_image = np.copy(image)\n",
    "    for i in range(image.shape[1]):\n",
    "        #print(image.shape[1])\n",
    "        \n",
    "        if (detector_id == 'nrs1'):\n",
    "            corrected_image[:, i] -= np.nanmedian( np.concatenate((corrected_image[y_lo_1:y_hi_1,i],corrected_image[y_lo_2:y_hi_2,i])) )\n",
    "\n",
    "        if (detector_id == 'nrs2'):\n",
    "            if (i <= 1250):\n",
    "                corrected_image[:, i] -= np.nanmedian( corrected_image[y_lo_1:y_hi_1,i] )\n",
    "            if (i > 1250):\n",
    "                corrected_image[:, i] -= np.nanmedian( corrected_image[y_lo_2:y_hi_2,i] )\n",
    "\n",
    "    return corrected_image      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985bd04-f1db-4df8-ace7-8f3c09813bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the background regions for each detector\n",
    "\n",
    "if process_detector == 'nrs1':\n",
    "    blow  = (0, 26)\n",
    "    bhigh = (5, 31)\n",
    "\n",
    "if process_detector == 'nrs2':\n",
    "    blow  = (26, 0)\n",
    "    bhigh = (31, 5)\n",
    "\n",
    "corrected_1overf_2Dspec = np.copy(all_spec)\n",
    "\n",
    "#print(len(corrected_rampfit_results[0,:,0]))\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(all_spec[:,10,10]))):\n",
    "    corrected_1overf_2Dspec[i, :, :] = correct_one_over_f( all_spec[i, :, :], blow, bhigh, process_detector )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12c65b-9af1-4910-af19-0381451ba23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if process_detector == 'nrs1':\n",
    "    int_id  = 0 # select an integration to show\n",
    "    f_lo = -0.5 # lower limit for flux\n",
    "    f_hi = +0.5 # upper limit for flux\n",
    "\n",
    "if process_detector == 'nrs2':\n",
    "    int_id  = 0 # select an integration to show\n",
    "    f_lo = -0.5 # lower limit for flux\n",
    "    f_hi = +0.5 # upper limit for flux\n",
    "\n",
    "# Plot the original uncorrected image\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(211)\n",
    "plt.title('Uncorrected 2D spectrum ' + process_detector.upper(), fontsize=20)\n",
    "im1 = plt.imshow(all_spec[int_id,:,:], \n",
    "                 interpolation='None', \n",
    "                 aspect='auto', \n",
    "                 cmap='inferno', \n",
    "                 origin='lower',\n",
    "                 clim=(f_lo, f_hi))\n",
    "\n",
    "plt.xlabel('x-column', fontsize=15)\n",
    "plt.ylabel('y-row', fontsize=15)\n",
    "\n",
    "cb1 = plt.colorbar(label=r'Counts, DN/s')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot the corrected image\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(211)\n",
    "plt.title('1/f-corrected 2D spectrum ' + process_detector.upper(), fontsize=20)\n",
    "im1 = plt.imshow(corrected_1overf_2Dspec[int_id,:,:], \n",
    "                 interpolation='None', \n",
    "                 aspect='auto', \n",
    "                 cmap='inferno', \n",
    "                 origin='lower',\n",
    "                 clim=(f_lo, f_hi))\n",
    "\n",
    "plt.xlabel('x-column', fontsize=15)\n",
    "plt.ylabel('y-row', fontsize=15)\n",
    "\n",
    "cb1 = plt.colorbar(label=r'Counts, DN/s')\n",
    "\n",
    "\n",
    "\n",
    "# Plot the difference image\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(211)\n",
    "plt.title('Difference 2D spectrum ' + process_detector.upper(), fontsize=20)\n",
    "im1 = plt.imshow(all_spec[int_id,:,:] - corrected_1overf_2Dspec[int_id,:,:], \n",
    "                 interpolation='None', \n",
    "                 aspect='auto', \n",
    "                 cmap='inferno', \n",
    "                 origin='lower',\n",
    "                 clim=(f_lo, f_hi))\n",
    "\n",
    "plt.xlabel('x-column', fontsize=15)\n",
    "plt.ylabel('y-row', fontsize=15)\n",
    "\n",
    "cb1 = plt.colorbar(label=r'Counts, DN/s')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15678c3-00e2-4fd1-b0c5-2e6f82825119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the 2D cutouts, so we don't have to run this step every time\n",
    "print('\\nSvaing ' + process_detector + preprocessed_file_names[2] + '.')\n",
    "    \n",
    "spectra2D_1overf = {\\\n",
    "'corrected_1overf_2Dspec':corrected_1overf_2Dspec\n",
    "}\n",
    "\n",
    "pickle_out = open( spec2_results + '/' + process_detector + preprocessed_file_names[2], 'wb')\n",
    "pickle.dump(spectra2D_1overf, pickle_out)\n",
    "pickle_out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af60b23-ae30-468b-afee-adbeac7bfbf3",
   "metadata": {},
   "source": [
    "# <font color='white'>-</font>Step 4: Identify and replace bad pixels <a class=\"anchor\" id=\"bad\"></a>\n",
    "\n",
    "A visual inspection of the 2D spectra reveals several isolated bright and dark pixels. Some of these pixels can be intrinsic cosmic rays, or snow balls, but in their majority these are bad e.g., hot or dead pixels. JWST data has flags that define such pixels. \n",
    "\n",
    "We can leave these pixels and continue our analysis. However, when it comes to centroiding and tracing the spectra, it is better to assign some values to these bad pixels, because they can confuse the fit for the centroid. In the step below, we will identify and replace these pixels. To do that, we will iterate over each column, identify bad pixels using the data quality extension for each integration and replace the bad pixels. The replacement will be done using a representative PSF profile, aka nominal PSF profile. It is constructed using the data by median combining a few adjacent columns that preceed and follow the column of inspection during the iteration. This method has been detailed in <a href=\"https://ui.adsabs.harvard.edu/abs/2014MNRAS.437...46N/abstract\">Nikolov et al. (2014).</a>\n",
    "\n",
    "In a second pass, we will take the difference between the nominal PSF profile and the relevant column. We will inspec the difference column for 3 sigma outliers and correct their values with the nominal PSF profile. This step would identify pixels that significantly deviate from the rest, but have not been identified and included in the data quality flags as bad/hot pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f25b54-2f85-41bf-bd26-f683a4788844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nominal_psf_prof(spec2D):\n",
    "    # Computes a nominal PSF profile for a 2D spectrum using median\n",
    "    # For best results: provide a few columns before and after e.g., 3-before and 3-after (6) \n",
    "    # the column for which a nominal PSF profile is needed\n",
    "    nominal_profile = np.nanmedian(spec2D, axis=1)\n",
    "    #print(nominal_profile)\n",
    "    return nominal_profile\n",
    "\n",
    "#nm_pr = nominal_psf_prof(all_spec[0,:,180:220])\n",
    "#print('nm_pr ', nm_pr, 'len ', len(nm_pr))\n",
    "\n",
    "\n",
    "def idx_for_nominal_psf_prof(row_id, xsize2Dspec, edge_min):\n",
    "    # Determines the column indexes to be used in the function nominal_psf_prof\n",
    "    # Input:\n",
    "    #       row_id - row index for which pixels for the nominal PSF profiles need to be determined\n",
    "    #       xsize2Dspec - size of the full two dimensional spectrum \n",
    "    # for which we need to compute nominal PSF profiles for each column\n",
    "    #       edge_min - width in px for the edges of the 2Dspec; if row_id is in the first\n",
    "    # or last edge_min pixels, then the pixels for nominal profile will be edge_min\n",
    "    \n",
    "    # 1: First few pixels\n",
    "    if ((row_id > -1) and (row_id <= 2*edge_min)):\n",
    "        idx = np.arange(0, edge_min)\n",
    "\n",
    "    # 2: Last few pixels\n",
    "    if ((row_id >= xsize2Dspec-2*edge_min) and (row_id < xsize2Dspec)):\n",
    "        idx = xsize2Dspec-edge_min + np.arange(edge_min)\n",
    "\n",
    "    # 3: Between 1 and 2\n",
    "    if ((row_id > 2*edge_min) and (row_id < xsize2Dspec-2*edge_min)):\n",
    "        idx = np.arange(0, edge_min*2+1) - edge_min\n",
    "        idx += row_id\n",
    "\n",
    "    if ((row_id <= -1) or (row_id >= xsize2Dspec)):\n",
    "        print('Warning: incorrectly provided index!!! row_id: ', row_id )\n",
    "        idx = np.arange(0, 1)-1\n",
    "        print('Hello!', idx)\n",
    "\n",
    "    return idx\n",
    "\n",
    "#xsize2Dspec = 1800 \n",
    "#edge_min = 3\n",
    "#print(idx_for_nominal_psf_prof(1500, xsize2Dspec, edge_min))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2672076-e660-4419-9bbf-b5d6af8acd0d",
   "metadata": {},
   "source": [
    "This step takes about 2.5 min to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49583652-f8cb-4191-bd17-d6c792ec7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can decide whether we wish to continue working with the corrected 1/f data, or the uncorrected data\n",
    "\n",
    "if (run_mode == 'process'):\n",
    "    start = tt.time()\n",
    "    \n",
    "    use_1overf_data = 'yes'\n",
    "    \n",
    "    if (use_1overf_data == 'yes'):\n",
    "        cor_all_spec = np.copy(corrected_1overf_2Dspec)\n",
    "        cp_2Dspec = np.copy(corrected_1overf_2Dspec)\n",
    "    \n",
    "    if (use_1overf_data == 'no'):\n",
    "        cor_all_spec = np.copy(all_spec)\n",
    "        cp_2Dspec = np.copy(all_spec)\n",
    "    \n",
    "    \n",
    "    nint = cp_2Dspec.shape[0]#-1\n",
    "    #spec_xlen = len(all_spec[0,0,:])\n",
    "    #spec_ylen = len(all_spec[0,:,0])\n",
    "    #print(nint, spec_xlen, spec_ylen)\n",
    "    \n",
    "    radius_edge_comb = 4 # how many columns to be combined at the edge of the detector; \n",
    "    \n",
    "    # Replace nans with median PSF profile pixels\n",
    "    # do it for all 2D spectra\n",
    "    for i in tqdm(range(nint)):        \n",
    "        for j in range(spec_xlen):\n",
    "        # do it for each column\n",
    "        #for j in range(30):\n",
    "            \n",
    "            # First check for nans in the evaluated column\n",
    "            # determine nan pixels in the PSF profile\n",
    "            #print(j)\n",
    "            idx_nan = np.where(np.isnan(cp_2Dspec[i,:,j]))[0]\n",
    "            idx_bad = np.where(all_DQs[i,:,j] > 0)[0]\n",
    "            #print('idx_bad ', idx_bad, 'idx_nan ', idx_nan)\n",
    "            \n",
    "            idx_nan_bad = np.concatenate((idx_nan,idx_bad))\n",
    "            idx_nan_bad = np.unique(idx_nan_bad)\n",
    "            #print('idx_nan_bad', idx_nan_bad)\n",
    "            \n",
    "            #print(idx_nan, len(idx_nan))\n",
    "    \n",
    "            # Proceed with columns with nans only (this saves some time)\n",
    "            if ((len(idx_nan_bad) >= 0)):\n",
    "    \n",
    "                #if ((i >= 75) and (i <= 79)):\n",
    "                #    print(i, idx_nan_bad, len(idx_nan_bad), j, spec_xlen, len(all_spec[i,:,j])-1, len(all_spec[i,0,:])-1)\n",
    "    \n",
    "                #print('j ', j, 'spec_xlen: ', spec_xlen )\n",
    "                # determine columns for the nominal PSF profile\n",
    "                idxp = idx_for_nominal_psf_prof(j, spec_xlen, edge_min=radius_edge_comb)\n",
    "                #print('j', j, ' idxp: ', idxp, ' last idx ', idxp[len(idxp)-1])\n",
    "                #print('idx_nan_bad: ', idx_nan_bad)\n",
    "                \n",
    "    \n",
    "                # compute the nominal profile \n",
    "                nm_pr = nominal_psf_prof(cp_2Dspec[i,:,idxp[0]:idxp[len(idxp)-1]])\n",
    "                #print(idx)\n",
    "                #print(idx[0], idx[edge_min*2])\n",
    "                #print('nm_pr ', nm_pr)\n",
    "                #print('nm_pr[idx_nan_bad] ', nm_pr[idx_nan_bad])\n",
    "    \n",
    "                # Take a difference between the nominal profile and currnet column and identfy outliers\n",
    "                prof_diff = cp_2Dspec[i,:,j] - nm_pr\n",
    "                #print('prof_diff', prof_diff)\n",
    "                mean_diff = np.nanmedian(prof_diff)\n",
    "                #print('mean_diff ', mean_diff)\n",
    "                std_diff = np.nanstd(prof_diff)\n",
    "                #print('std_diff ', std_diff)\n",
    "                idx_lo = np.where(prof_diff <= mean_diff-3.0*std_diff)[0]\n",
    "                idx_hi = np.where(prof_diff >= mean_diff+3.0*std_diff)[0]\n",
    "    \n",
    "                idx_all = np.concatenate((idx_nan_bad, idx_lo, idx_hi))\n",
    "                idx_rem = np.unique(idx_all)\n",
    "                \n",
    "                # Replace the pixels with nans with the corresponding values of the nominal PSF profile\n",
    "                cor_all_spec[i,idx_rem,j] = nm_pr[idx_rem]\n",
    "                #cor_all_spec[i, idx_nan, j] = nm_pr[idx_nan]            \n",
    "                #cor_all_spec[i,:,j] = nm_pr\n",
    "                #print(nm_pr[idx_nan_bad])\n",
    "    \n",
    "    print('Step done!')\n",
    "    end = tt.time()\n",
    "    print(\"Run time: \", round(end-start,1)/60.0, \" min\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c16096-6da5-4b56-bea5-773689397965",
   "metadata": {},
   "source": [
    "In the next cell, we load pre-processed data, in case we run this notebook in 'load' mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b29c5-327d-4481-8d88-87f6c7ca8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the pickle file with the nominal PSF corrected data\n",
    "if (run_mode == 'load'):\n",
    "    \n",
    "    print('\\nLoading ' + process_detector + preprocessed_file_names[3] + '.')\n",
    "\n",
    "    \n",
    "    pickle_in = open( spec2_results + '/' + process_detector + preprocessed_file_names[3], 'rb')\n",
    "    \n",
    "    nominal_psf = pickle.load( pickle_in )\n",
    "    pickle_in.close() # close the file\n",
    "    \n",
    "    print('\\nShowing the pickled data:')\n",
    "    \n",
    "    cnt = 0\n",
    "    for item in nominal_psf:\n",
    "        print('The data ', cnt, ' is : ', item)\n",
    "        cnt += 1\n",
    "    \n",
    "    cor_all_spec = nominal_psf['cor_all_spec']\n",
    "    radius_edge_comb = nominal_psf['radius_edge_comb']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf537e2a-5c03-4263-bf73-c2fbaec7a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the image with replaced nans for inspection\n",
    "\n",
    "if process_detector == 'nrs1':\n",
    "    f_lo = -20 # lower limit for flux\n",
    "    f_hi = +20 # upper limit for flux\n",
    "\n",
    "if process_detector == 'nrs2':\n",
    "    f_lo = -20 # lower limit for flux\n",
    "    f_hi = +20 # upper limit for flux\n",
    "\n",
    "\n",
    "if (nint <= 2):\n",
    "    nshow = 0\n",
    "else:\n",
    "    nshow = 250\n",
    "\n",
    "\n",
    "# First, plot an uncorrected 2D spectrum \n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(211)\n",
    "\n",
    "\n",
    "plt.title('Spectrum BEFORE nan replacement', fontsize=20)\n",
    "im1 = plt.imshow(all_spec[nshow,:,:], \n",
    "                 interpolation='None', \n",
    "                 aspect='auto', \n",
    "                 cmap='inferno', \n",
    "                 origin='lower',\n",
    "                 clim=(f_lo, f_hi))\n",
    "\n",
    "plt.xlabel('x-column', fontsize=15)\n",
    "plt.ylabel('y-row', fontsize=15)\n",
    "\n",
    "cb1 = plt.colorbar(label=r'Counts, DN/s')\n",
    "\n",
    "\n",
    "\n",
    "# Second, plot a corrected 2D spectrum \n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(211)\n",
    "\n",
    "plt.title('Spectrum AFTER nan replacement', fontsize=20)\n",
    "im1 = plt.imshow(cor_all_spec[nshow,:,:], \n",
    "                 interpolation='None', \n",
    "                 aspect='auto', \n",
    "                 cmap='inferno', \n",
    "                 origin='lower',\n",
    "                 clim=(f_lo, f_hi))\n",
    "\n",
    "plt.xlabel('x-column', fontsize=15)\n",
    "plt.ylabel('y-row', fontsize=15)\n",
    "\n",
    "cb1 = plt.colorbar(label=r'Counts, DN/s')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c05bf6-6cf4-4759-9a75-72cf11c14b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pickle the corrected data, so we don't have to run it every time (this cells will execute if we run the notebook in 'process' mode)\n",
    "if (run_mode == 'process'):\n",
    "    \n",
    "    print('\\nSaving ' + process_detector + preprocessed_file_names[3] + '.')\n",
    "\n",
    "    nominal_psf = {\\\n",
    "    'cor_all_spec':cor_all_spec, \\\n",
    "    'radius_edge_comb':radius_edge_comb \\\n",
    "    }\n",
    "    \n",
    "    pickle_out = open( spec2_results + '/' + process_detector + preprocessed_file_names[3], 'wb')\n",
    "    pickle.dump(nominal_psf, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd45725-2519-43c7-a072-f27cfb3f4de9",
   "metadata": {},
   "source": [
    "# <font color='white'>-</font>Step 5: Obtain centroid information using each 2D spectrum <a class=\"anchor\" id=\"centroid\"></a>\n",
    "\n",
    "At this step, we will fit a Gaussian model to the spatial profile (column) of the 2d spectrum to determine where the spectrum is located for each column, and will obtain FWHM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161c131-5f7f-4c8b-9461-cdcd10292e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectral_centroid(image, dqflags, xstart, xend, plot_input_spec, title_in, plot_gauss, plot_trace):\n",
    "    # Trace a spectrum (non-defocused)\n",
    "    \n",
    "    if plot_input_spec == 'yes':\n",
    "        #Plot the median spectrum\n",
    "        plt.figure(figsize=(10,2))\n",
    "        plt.subplot(211)\n",
    "        plt.title(title_in)\n",
    "\n",
    "        im = plt.imshow(image, \n",
    "                 interpolation='None', \n",
    "                 aspect='auto', \n",
    "                 cmap='inferno', \n",
    "                 origin='lower',\n",
    "                 clim=(f_lo, f_hi))\n",
    "\n",
    "        plt.xlabel('X-column, pixel')\n",
    "        plt.ylabel('Y-row, pixel')\n",
    "\n",
    "        cb = plt.colorbar()\n",
    "        cb.ax.set_ylabel('Rates (Counts/s)')\n",
    "\n",
    "    \n",
    "    # Prepare arrays that will save our results:\n",
    "    x = np.arange(0, xend-xstart)\n",
    "    y = np.zeros([xend-xstart])\n",
    "    fwhm = np.zeros([xend-xstart])\n",
    "\n",
    "    #Fit a Gaussian to the spectral profile of the median image    \n",
    "    for j in range(len(image[0,:])-1):\n",
    "        #print(j+1, ' out of ', len(image[0,:]))\n",
    "        \n",
    "        prof_yy = image[:,j]\n",
    "        prof_xx = np.arange(len(prof_yy))\n",
    "\n",
    "        idx_good = np.where((dqflags[:, j] == 0) & (~np.isnan(image[:, j])))[0]\n",
    "        idx_bad = np.where((dqflags[:, j] != 0) & (np.isnan(image[:, j])))[0]\n",
    "        \n",
    "        #print(len(prof_xx), 'XX')\n",
    "        #print(len(prof_yy), 'YY')\n",
    "\n",
    "        #print(idx_good)\n",
    "        \n",
    "        #prof_xx = prof_xx[idx_good]\n",
    "        #prof_yy = prof_yy[idx_good]\n",
    "\n",
    "        prof_yy[np.isnan(prof_yy)] = 0.\n",
    "\n",
    "        mean_yy = np.nanmean(prof_yy)\n",
    "        std_yy = np.nanstd(prof_yy)\n",
    "        \n",
    "        ycen = np.mean(np.where(prof_yy >= mean_yy+2.0*std_yy)[0])\n",
    "        #print(ycen)\n",
    "        g_init = models.Gaussian1D(amplitude=np.nanmax(prof_yy), mean=ycen, stddev=2.)\n",
    "\n",
    "        fit_g = fitting.LevMarLSQFitter()\n",
    "        #print(len(prof_xx), len(prof_yy))\n",
    "        g = fit_g(g_init, prof_xx, prof_yy)\n",
    "        y[j] = g.mean.value\n",
    "        fwhm[j] = g.stddev.value\n",
    "\n",
    "\n",
    "        #Plot the best-fit Gaussian for one column\n",
    "        if plt_gauss == 'yes':\n",
    "            if j == 0:\n",
    "                plt.figure(figsize=(5,5))\n",
    "                plt.title(\"BFit Gauss to y-col\")\n",
    "                plt.plot(prof_xx, prof_yy, 'ko')\n",
    "                plt.plot(prof_xx, g(prof_xx), label='Gaussian Fit')\n",
    "                plt.xlabel('y-position, px')\n",
    "                plt.ylabel('Flux, Counts/s')\n",
    "                \n",
    "    #Plot the measured trace for the median image\n",
    "    if plt_trace == 'yes':\n",
    "        plt.figure(figsize=(10,2))\n",
    "        plt.subplot(211)\n",
    "        plt.title(title_in+\" and centroid (red line)\")\n",
    "        plt.xlabel('X-column, pixel')\n",
    "        plt.ylabel('Y-row, pixel')\n",
    "        fx_scale, fy_scale = 20, 8\n",
    "        img_rescale = cv2.resize(image, (0, 0), fx=fx_scale, fy=fy_scale)\n",
    "        #img_rescale = cv2.resize(image)\n",
    "        im = plt.imshow(img_rescale, interpolation='None', aspect='auto', cmap='inferno', origin='lower', clim=(f_lo, f_hi))\n",
    "        #im.set_clim(-3, 3)\n",
    "        cb = plt.colorbar()\n",
    "        cb.ax.set_ylabel('Rates (Counts/s)')\n",
    "        plt.plot(fx_scale*x, fy_scale*y, color='red', lw=1, alpha=0.9)\n",
    "        plt.show()\n",
    "    return x, y, fwhm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7156b30-7180-41fb-989b-2934007844c5",
   "metadata": {},
   "source": [
    "This step takes about 13 and 18 minutes to run on NRS1 and NRS2, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d2501-660f-4190-ba5b-fb401d9914b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will use the 1/f and nan-corrected data to perform our analysis\n",
    "use_corrected_data = 'yes'\n",
    "\n",
    "if (run_mode == 'process'):\n",
    "    \n",
    "    \n",
    "    if (use_corrected_data == 'yes'):\n",
    "        cp_2Dspec = np.copy(cor_all_spec)\n",
    "    \n",
    "    if (use_corrected_data == 'no'):\n",
    "        cor_all_spec = np.copy(all_spec)\n",
    "        cp_2Dspec = np.copy(all_spec)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    use_median = 'no' # yes/no\n",
    "    \n",
    "    if (use_median == 'yes'):\n",
    "        # Compute a median combined spectrum\n",
    "        #Compute a median image\n",
    "        idx1 = 0\n",
    "        idx2 = 20\n",
    "        median_image = np.median(cp_2Dspec[idx1:idx2,:,:], axis=0)\n",
    "    \n",
    "        # Define parameters for the centroiding algorithm:\n",
    "        xstart = 0                        #start of spectrum on the detector\n",
    "        xend = len(median_image[0,0:])-0       # end of spectrum on the detector\n",
    "        #print(xend)\n",
    "    \n",
    "        #ystart = 20 #define mid-line for tracing\n",
    "        plot_input_spec = 'yes'\n",
    "        title_in        = 'Median Image'\n",
    "        plt_gauss       = 'yes'\n",
    "        plt_trace       = 'yes'\n",
    "    \n",
    "        # Prepare arrays that will save our results:\n",
    "        nintegrations = 1\n",
    "        print(nintegrations)\n",
    "    \n",
    "        X1 = np.zeros([nintegrations, xend-xstart])\n",
    "        Y1 = np.zeros([nintegrations, xend-xstart])\n",
    "        FWHM1 = np.zeros([nintegrations, xend-xstart])\n",
    "    \n",
    "        # Obtain centroid information:\n",
    "        for i in range(nintegrations):\n",
    "            X1[i,:], Y1[i,:], FWHM1[i,:] = get_spectral_centroid(median_image, all_DQs[0,:,:], xstart, xend, plot_input_spec, title_in, plt_gauss, plt_trace )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if (use_median == 'no'):\n",
    "    \n",
    "        # Define parameters for the centroiding algorithm:\n",
    "        xstart = 0                        #start of spectrum on the detector\n",
    "        xend = spec_xlen #len(median_image[0,0:])-0       # end of spectrum on the detector\n",
    "        #print(xend)\n",
    "    \n",
    "        #ystart = 20 #define mid-line for tracing\n",
    "        plot_input_spec = 'no'\n",
    "        title_in        = 'Image'\n",
    "        plt_gauss       = 'no'\n",
    "        plt_trace       = 'no'\n",
    "    \n",
    "        # Prepare arrays that will save our results:\n",
    "        nintegrations = nint# all_spec.shape[0]\n",
    "        print(nintegrations)\n",
    "    \n",
    "        X1 = np.zeros([nintegrations, xend-xstart])\n",
    "        Y1 = np.zeros([nintegrations, xend-xstart])\n",
    "        FWHM1 = np.zeros([nintegrations, xend-xstart])\n",
    "        #print(len(X1[:,0]), len(Y1[0,:]))\n",
    "        \n",
    "        # Let's time the step runtime\n",
    "        start = tt.time()\n",
    "        \n",
    "        # Obtain centroid information:\n",
    "        for i in tqdm(range(nintegrations)):\n",
    "            X1[i,:], Y1[i,:], FWHM1[i,:] = get_spectral_centroid(cp_2Dspec[i,:,:], all_DQs[i,:,:], xstart, xend, plot_input_spec, title_in, plt_gauss, plt_trace )\n",
    "            \n",
    "        end = tt.time()\n",
    "        print(\"Run time: \", round(end-start,1)/60.0, \" min\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8343b6-1c93-432f-b7a8-184ee7fb77ca",
   "metadata": {},
   "source": [
    "The arrays X1, Y1 and FWHM contain the centroid information for each integration. Instead of plotting them here, we will save them and use them later for the spectral tracing and to plot their median value for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b906e-c1ba-43e8-a035-2d2982ecc8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pickle the centroiding data along with the data, so we don't have to run it every time\n",
    "if (run_mode == 'process'):\n",
    "    \n",
    "    print('\\nSaving ' + process_detector + preprocessed_file_names[4] + '.')\n",
    "    \n",
    "    centroiding_data = {\\\n",
    "    'X1':X1, \\\n",
    "    'Y1':Y1, \\\n",
    "    'FWHM1':FWHM1 \\\n",
    "    }\n",
    "    \n",
    "    #if process_detector == 'nrs1':\n",
    "    pickle_out = open( spec2_results + process_detector + preprocessed_file_names[4], 'wb')\n",
    "    pickle.dump(centroiding_data, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c24138-4889-42ec-b300-5d87bcf34209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Restore the centroiding data\n",
    "if (run_mode == 'load'):\n",
    "    \n",
    "    print('\\nLoading ' + process_detector + preprocessed_file_names[4] + '.')\n",
    "    \n",
    "    pickle_in = open( spec2_results + '/' + process_detector + preprocessed_file_names[4], 'rb')\n",
    "    \n",
    "    centroiding_data = pickle.load( pickle_in )\n",
    "    pickle_in.close() # close the file\n",
    "    \n",
    "    print('\\nShowing the pickled data:')\n",
    "    \n",
    "    cnt = 0\n",
    "    for item in centroiding_data:\n",
    "        print('The data ', cnt, ' is : ', item)\n",
    "        cnt += 1\n",
    "    \n",
    "    X1 = centroiding_data['X1']\n",
    "    Y1 = centroiding_data['Y1']\n",
    "    FWHM1 = centroiding_data['FWHM1']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22412d6b-417f-42ed-8325-6f16be2a829d",
   "metadata": {},
   "source": [
    "# <font color='white'>-</font>Step 6: Spectral tracing <a class=\"anchor\" id=\"tracing\"></a>\n",
    "\n",
    "This step takes the centroiding data from the previous step and fits a polynomial to it to better locate the spectrum. The trace information is used to extract the spectra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e3d5b5-51e8-4010-8a03-71a73de85bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a Chebyshev polynomial to the centroid data\n",
    "# Try orders from 1 to 30 in the polynomial for all the traces:\n",
    "# Use the best-one as deemed by the BIC to fit all the traces; plot them with median image on top:\n",
    "\n",
    "\n",
    "use_corrected_data = 'yes'\n",
    "\n",
    "if (use_corrected_data == 'yes'):\n",
    "    cp_2Dspec = np.copy(cor_all_spec)\n",
    "\n",
    "if (use_corrected_data == 'no'):\n",
    "    cp_2Dspec = np.copy(all_spec)\n",
    "\n",
    "\n",
    "\n",
    "nintegrations = cp_2Dspec.shape[0]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(211)\n",
    "\n",
    "im = plt.imshow(cp_2Dspec[150,:,:], \n",
    "         interpolation='None', \n",
    "         aspect='auto', \n",
    "         cmap='inferno', \n",
    "         origin='lower',\n",
    "         clim=(f_lo, f_hi))\n",
    "\n",
    "plt.xlabel('X-column, pixel')\n",
    "plt.ylabel('Y-row, pixel')\n",
    "\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_ylabel('Rates (Counts/s)')\n",
    "\n",
    "order = 2\n",
    "coeffs1 = np.zeros([nintegrations, order+1])\n",
    "\n",
    "SP_TRACE_X = 0.*np.copy(FWHM1)\n",
    "SP_TRACE_Y = 0.*np.copy(FWHM1)\n",
    "\n",
    "for i in range(nintegrations):\n",
    "    # Fit only non-nans:\n",
    "    x1_px = 10\n",
    "    x2_px = len(Y1[0,:])-10\n",
    "    idx = np.where(~np.isnan(Y1[i,x1_px:x2_px]))\n",
    "    coeffs1[i,:] = chebyshev.chebfit(X1[i,x1_px:x2_px][idx], Y1[i,x1_px:x2_px][idx], deg=order)\n",
    "    #plt.plot(X1[i,x1_px:x2_px], chebyshev.chebval(X1[i,x1_px:x2_px], coeffs1[i,:x2_px]), lw=1, color='blue')\n",
    "    plt.plot(X1[i,:], chebyshev.chebval(X1[i,:], coeffs1[i,:]), lw=1, color='blue')\n",
    "    plt.title('2D spectrum and trace', fontsize=20)\n",
    "    SP_TRACE_X[i,:] = X1[i,:]\n",
    "    SP_TRACE_Y[i,:] = chebyshev.chebval(X1[i,:], coeffs1[i,:])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a4fce-d326-4f7a-b6f1-1be2ffa848e2",
   "metadata": {},
   "source": [
    "OK, the spectral trace goes right in the middle of the spectrum.\n",
    "Let's obtain median values for the FWHM and centroiding data using several pixel columns, and plot them as a function of time. Such data is sensistve to any changes in the conditions of the telescope and instrument during the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa09b2-bbc8-45e6-9324-739ce24ac717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute median FWHM per spectrum\n",
    "median_fwhm = np.zeros(nintegrations)\n",
    "median_yshift = np.zeros(nintegrations)\n",
    "median_sp_trace_y = np.zeros(nintegrations)\n",
    "\n",
    "x_lo = 500\n",
    "x_hi = 1300#spec_xlen - 10\n",
    "\n",
    "print(nintegrations)\n",
    "\n",
    "for i in range(nintegrations):\n",
    "    median_fwhm[i] = np.nanmedian(FWHM1[i,x_lo:x_hi])\n",
    "    median_yshift[i] = np.nanmedian(Y1[i,x_lo:x_hi])\n",
    "    median_sp_trace_y[i] = np.nanmedian(SP_TRACE_Y[i,x_lo:x_hi])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "sigma_fwhm = np.sqrt(np.var(median_fwhm))\n",
    "plt.plot((all_times-np.mean(all_times))*24., median_fwhm, color='red', label=f'FWHM of centroid, px (r.m.s.={round(sigma_fwhm, 3)} px)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Time since mid-exposure, hr', fontsize=15)\n",
    "plt.ylabel('median FWHM (pixel)', fontsize=15)\n",
    "#plt.xlim([1, nintegrations])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "sigma_yshift = np.sqrt(np.var(median_yshift))\n",
    "plt.plot((all_times-np.mean(all_times))*24., median_yshift, color='red', label=f'y-location of centroid, px (r.m.s.={round(sigma_yshift, 3)} px)')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Time since mid-exposure, hr', fontsize=15)\n",
    "plt.ylabel('median_yshift (pixel)', fontsize=15)\n",
    "#plt.xlim([1, nintegrations])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "sigma_ytrace = np.sqrt(np.var(median_sp_trace_y))\n",
    "plt.plot((all_times-np.mean(all_times))*24., median_sp_trace_y, color='red', label=f'y-trace of centroid, px (r.m.s.={round(sigma_ytrace, 3)} px)')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Time since mid-exposure, hr', fontsize=15)\n",
    "plt.ylabel('median_sp_trace (pixel)', fontsize=15)\n",
    "#plt.xlim([1, nintegrations])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012dc426-15e4-4818-b4d5-c18a038c07f5",
   "metadata": {},
   "source": [
    "We clearly see a jump in the FWHM during the transit. Such abrupt changes are atributed to 'Tilt' events. The term 'Tilt' event refers to any un-commanded change of a primary mirror segment tilt and/or piston position. Such uncommanded changes can for instance be caused by micro-meteorites that encounter JWT primary mirror. For more details, please consult the following pages:\n",
    "\n",
    "<a href=\"https://jwst-docs.stsci.edu/methods-and-roadmaps/jwst-time-series-observations/jwst-time-series-observations-noise-sources#JWSTTimeSeriesObservationsNoiseSources-%22Tilt%22events\">1. JWST Time-Series Observations Noise Sources</a>\n",
    "<br>\n",
    "<a href=\"https://jwst-docs.stsci.edu/jwst-observatory-characteristics/jwst-micrometeoroid-avoidance-zone\">2. JWST Micrometeoroid Avoidance Zone</a>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36306b00-8669-4e63-a8b2-19fb1c08da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pickle the trace data, so we don't have to run it every time\n",
    "if (run_mode == 'process'):\n",
    "    \n",
    "    print('\\nSaving ' + process_detector + preprocessed_file_names[5] + '.')\n",
    "    \n",
    "    spectral_trace = {\\\n",
    "    'SP_TRACE_X':SP_TRACE_X, \\\n",
    "    'SP_TRACE_Y':SP_TRACE_Y, \\\n",
    "    'coeffs1':coeffs1, \\\n",
    "    'order':order, \\\n",
    "    'median_fwhm':median_fwhm, \\\n",
    "    'median_yshift':median_yshift, \\\n",
    "    'median_sp_trace_y':median_sp_trace_y, \\\n",
    "    'x_lo':x_lo, \\\n",
    "    'x_hi':x_hi\n",
    "    }\n",
    "    \n",
    "    #if process_detector == 'nrs1':\n",
    "    pickle_out = open( spec2_results + process_detector + preprocessed_file_names[5], 'wb')\n",
    "    pickle.dump(spectral_trace, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5681050-9097-40e3-a5cf-bb267f3c4093",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Restore the spectral trace\n",
    "if (run_mode == 'load'):\n",
    "    \n",
    "    print('\\nLoading ' + process_detector + preprocessed_file_names[5] + '.')\n",
    "    \n",
    "    pickle_in = open( spec2_results + process_detector + preprocessed_file_names[5], 'rb')\n",
    "    \n",
    "    spectral_trace = pickle.load( pickle_in )\n",
    "    pickle_in.close() # close the file\n",
    "    \n",
    "    print('Showing the pickled data:')\n",
    "    \n",
    "    cnt = 0\n",
    "    for item in spectral_trace:\n",
    "        print('The data ', cnt, ' is : ', item)\n",
    "        cnt += 1\n",
    "    \n",
    "    SP_TRACE_X = spectral_trace['SP_TRACE_X']\n",
    "    SP_TRACE_Y = spectral_trace['SP_TRACE_Y']\n",
    "    coeffs1 = spectral_trace['coeffs1']\n",
    "    order = spectral_trace['order']\n",
    "    median_fwhm = spectral_trace['median_fwhm']\n",
    "    median_yshift = spectral_trace['median_yshift']\n",
    "    median_sp_trace_y = spectral_trace['median_sp_trace_y']\n",
    "    x_lo = spectral_trace['x_lo']\n",
    "    x_hi = spectral_trace['x_hi']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e25811-a114-492d-875c-ab216d4e4dbb",
   "metadata": {},
   "source": [
    "# <font color='white'>-</font>Step 7: Extract the spectra and obtain wavelength solution <a class=\"anchor\" id=\"extraction\"></a>\n",
    "\n",
    "To obtain the light curves, we first need to extract one-dimensional spectra (1d) and assign wavelengths to each pixel. We will perform a simple aperture extraction using the trace information from the previous step. For each trace (in fact to each integer of the trace position) we will sum the flux from the corrected (for 1/f noise and replaced bad pixels) 2d spectra inside the aperture. \n",
    "\n",
    "With regards to the spectral background, we also define a background region for each column and use it to determine the median value. The median background is then subtracted from each column. We note that the 1/f step performed above does a very similar reduction of the background compared to this step of the aperture extraction. Since we already removed the 1/f noise, this step also removed the median background from each column. Therefore, in the aperture step below, we skip the background subtraction step. It is left at the discretion of the user to compare results with this step enabled.\n",
    "\n",
    "The function below will perform the aperture extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005faa22-f138-4301-833b-24fb1faab18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aperture_extraction(image, x, y, aperture_radius, background_radius=15, error_image=None, correct_bkg=True):\n",
    "    \"\"\"\n",
    "    This function takes as inputs two arrays (x,y) that follow the trace, \n",
    "    and returns the added flux over the defined aperture radius (and its error, if an error image \n",
    "    is given as well), substracting in the way any background between the aperture radius and the \n",
    "    background radius. The background is calculated by taking the median of the points between the \n",
    "    aperture_radius and the background_radius.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image: ndarray\n",
    "        Image from which the spectrum wants to be extracted\n",
    "    x: ndarray\n",
    "        Array with the x-axis of the trace (i.e., the columns, wavelength direction)\n",
    "    y: ndarray\n",
    "        Array with the y-axis of the trace (i.e., rows, spatial direction)\n",
    "    aperture_radius: float\n",
    "        Distance from the center of the trace at which you want to add fluxes.\n",
    "    background_radius: float\n",
    "        Distance from the center of the trace from which you want to calculate the background. The \n",
    "        background region will be between this radius and the aperture_radius.\n",
    "    error_image: ndarray\n",
    "        Image with the errors of each pixel value on the image ndarray above\n",
    "    correct_bkg: boolean\n",
    "        If True, apply background correction. If false, ommit this.\n",
    "    \"\"\"\n",
    "    #print(np.ndim(image), len(x))\n",
    "    # Create array that will save our fluxes:\n",
    "    flux = np.zeros(len(x))\n",
    "    \n",
    "    if error_image is not None:\n",
    "        flux_error = np.zeros(len(x))\n",
    "        \n",
    "    max_column = image.shape[0]\n",
    "    for i in range(len(x)):\n",
    "\n",
    "        #print(int(x[i]))\n",
    "        \n",
    "        # Cut the column with which we'll be working with:\n",
    "        #print( int(x[i]), len(image[:,2]))\n",
    "        column = image[:,int(x[i])]\n",
    "        if error_image is not None:\n",
    "            variance_column = error_image[:,int(x[i])]**2\n",
    "            \n",
    "        # Define limits given by the aperture_radius and background_radius variables:\n",
    "        if correct_bkg:\n",
    "            left_side_bkg = np.max([y[i] - background_radius, 0])\n",
    "            right_side_bkg = np.min([max_column, y[i] + background_radius])\n",
    "        left_side_ap = np.max([y[i] - aperture_radius, 0])\n",
    "        right_side_ap = np.min([max_column, y[i] + aperture_radius])\n",
    "        \n",
    "        # Extract background, being careful with edges:\n",
    "        if correct_bkg:\n",
    "            bkg_left = column[np.max([0, int(left_side_bkg)]) : np.max([0, int(left_side_ap)])]\n",
    "            bkg_right = column[np.min([int(right_side_ap), max_column]) : np.max([int(right_side_bkg), max_column])]\n",
    "            bkg = np.median(np.append(bkg_left, bkg_right))\n",
    "        else:\n",
    "            bkg = 0.\n",
    "            \n",
    "        # Substract it from the column:\n",
    "        column -= bkg\n",
    "        \n",
    "        # Perform aperture extraction of the background-substracted column, being careful with pixelization \n",
    "        # at the edges. First, deal with left side:\n",
    "        l_decimal, l_integer = np.modf(left_side_ap)\n",
    "        l_integer = int(l_integer)\n",
    "        if l_decimal < 0.5:\n",
    "            l_fraction = (0.5 - l_decimal) * column[l_integer]\n",
    "            l_limit = l_integer + 1\n",
    "            if error_image is not None:\n",
    "                l_fraction_variance = ((0.5 - l_decimal)**2) * variance_column[l_integer]\n",
    "        else:\n",
    "            l_fraction = (1. - (l_decimal - 0.5)) * column[l_integer + 1]\n",
    "            l_limit = l_integer + 2\n",
    "            if error_image is not None:\n",
    "                l_fraction_variance = ((1. - (l_decimal - 0.5))**2) * variance_column[l_integer + 1]\n",
    "                \n",
    "        # Now right side:\n",
    "        r_decimal, r_integer = np.modf(right_side_ap)\n",
    "        r_integer = int(r_integer)\n",
    "        if r_decimal < 0.5:\n",
    "            r_fraction = (1. - (0.5 - r_decimal)) * column[r_integer]\n",
    "            r_limit = r_integer\n",
    "            if error_image is not None:\n",
    "                r_fraction_variance = ((1. - (0.5 - r_decimal))**2) * variance_column[r_integer]\n",
    "        else:\n",
    "            r_fraction = (r_decimal - 0.5) * column[r_integer + 1]\n",
    "            r_limit = r_integer + 1\n",
    "            if error_image is not None:\n",
    "                r_fraction_variance = ((r_decimal - 0.5)**2) * variance_column[r_integer + 1]\n",
    "                \n",
    "        # Save total flux in current column:\n",
    "        flux[i] = l_fraction + r_fraction + np.sum(column[l_limit:r_limit])\n",
    "        \n",
    "        if error_image is not None:\n",
    "            # For the flux error, ommit edge values (contribution to total variance is small nonetheless):\n",
    "            flux_error[i] = np.sqrt(np.sum(variance_column[l_limit:r_limit]) + l_fraction_variance + \\\n",
    "                                    r_fraction_variance)\n",
    "            \n",
    "    if error_image is not None:\n",
    "        return flux, flux_error\n",
    "    else:\n",
    "        return flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e68cf10-981e-4305-8e4c-ccbdfff87df2",
   "metadata": {},
   "source": [
    "Next, we just need to iterate over all of the 2d spectra and run the extraction function. This step is fast and takes only a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a2bc1-1b9d-40b4-ad04-a74b5aa9a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract spectra \n",
    "use_corrected_data = 'yes'\n",
    "\n",
    "if (use_corrected_data == 'yes'):\n",
    "    cp_2Dspec = np.copy(cor_all_spec)\n",
    "\n",
    "if (use_corrected_data == 'no'):\n",
    "    cp_2Dspec = np.copy(all_spec)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extraction parameters:\n",
    "if (process_detector == 'nrs1'):\n",
    "    extraction_aperture =  3 #8#30 this is radius\n",
    "    background_aperture = 10\n",
    "\n",
    "if (process_detector == 'nrs2'):\n",
    "    extraction_aperture =  5 #8#30 this is radius\n",
    "    background_aperture = 10\n",
    "    print(len(cp_2Dspec[0,0,:]), len(SP_TRACE_X[0,:]),len(SP_TRACE_Y[0,:]), len(all_err[0,0,:]) )\n",
    "    \n",
    "# Create dictionary:\n",
    "spectra = {}\n",
    "# Generate sub-dictionaries for each order:\n",
    "spectra['order1'] = {}\n",
    "# Save the X positions for both orders. X positions are the same for all integrations, so \n",
    "# we save the ones corresponding to the first integration:\n",
    "spectra['order1']['x'] = X1[0,:]\n",
    "\n",
    "# Create sub-dictionaries that will save the fluxes and the errors on those fluxes:\n",
    "spectra['order1']['flux'] = np.zeros([cp_2Dspec.shape[0], len(X1[0,:])])\n",
    "spectra['order1']['flux_errors']= np.zeros([cp_2Dspec.shape[0], len(X1[0,:])])\n",
    "\n",
    "# Now iterate through all integrations:\n",
    "for i in tqdm(range(nintegrations)):\n",
    "    # Trace order 1:\n",
    "    y1 = chebyshev.chebval(X1[0,:], coeffs1[i,:])\n",
    "    # Extract order 1:\n",
    "    #print(i, X1[i,2])\n",
    "    spectra['order1']['flux'][i,:], spectra['order1']['flux_errors'][i,:] = \\\n",
    "                                                         aperture_extraction(cp_2Dspec[i,:,:], SP_TRACE_X[i,:], SP_TRACE_Y[i,:], \n",
    "                                                                             extraction_aperture, \n",
    "                                                                             error_image=all_err[i,:,:], \n",
    "                                                                             correct_bkg=False)\n",
    "\n",
    "# Plot one extracted spectrum\n",
    "i = 5\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.set_title('NIRSpec spectrum, integration ' + str(i+1))\n",
    "ax.errorbar(spectra['order1']['x'], spectra['order1']['flux'][i,:], \\\n",
    "             yerr=spectra['order1']['flux_errors'][i,:], color='cornflowerblue')\n",
    "\n",
    "ax.set_xlim(np.min(spectra['order1']['x']), np.max(spectra['order1']['x']))\n",
    "\n",
    "ax.set_xlabel('Pixel column')\n",
    "ax.set_ylabel('Counts/s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816831d-fdd7-46fb-a889-eb68b6f6c8c6",
   "metadata": {},
   "source": [
    "In the next five lines print the first and last columns from the wavelength map for which the data values are not nans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbb11e-2674-4d6d-8f14-9c4789830404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (process_detector == 'nrs1'):\n",
    "    print(wavelength_map[:,0])\n",
    "    print(len(wavelength_map[4,:]))\n",
    "\n",
    "if (process_detector == 'nrs2'):\n",
    "    print(wavelength_map[:,2018])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(211)\n",
    "plt.title('Wavelength Map ' + process_detector.upper(), fontsize=20)\n",
    "im1 = plt.imshow(wavelength_map, \n",
    "                 interpolation='None', \n",
    "                 aspect='auto', \n",
    "                 cmap='inferno', \n",
    "                 origin='lower')\n",
    "\n",
    "plt.xlabel('x-column', fontsize=15)\n",
    "plt.ylabel('y-row', fontsize=15)\n",
    "\n",
    "cb1 = plt.colorbar(label=r'Wavelength, [$\\mu$m]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20431a-659f-42e8-9382-138bb5421f88",
   "metadata": {},
   "source": [
    "In the next cell, we will define minimum and maximum pixel indexes, which we will use to trim the data to avoid the nans in the wavelength map. Next, we obtain the wavelength solution by iterating over each column and taking the median of wavelengths inside the aperture that was used earlier to extract the spectra. We will then plot again the spectrum to make sure everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d5e05-66b0-491e-97ff-915fd2e740dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "spec_xlen = len(cp_2Dspec[0,0,:])\n",
    "wave_um = np.zeros(spec_xlen)\n",
    "#print(spec_xlen)\n",
    "\n",
    "#print(len(wavelength_map[0,:]))\n",
    "#print(len(wavelength_map[:,0]))\n",
    "#print(len(wave_um))\n",
    "\n",
    "# The wavelength map has some nans inside the spectral trace at the eadges. We romeve them by trimming these points.\n",
    "if (process_detector == 'nrs1'):\n",
    "    x11 = 10#126\n",
    "    x22 = 1382\n",
    "\n",
    "if (process_detector == 'nrs2'):\n",
    "    x11 = 8\n",
    "    x22 = 2018#len(all_spec[0,0,:]-2500)\n",
    "    #print(len(all_spec[0,0,:]))\n",
    "\n",
    "    \n",
    "for i in range(spec_xlen-1):\n",
    "    wave_um[i] = np.nanmedian(wavelength_map[ int(Y1[0,i])-extraction_aperture:int(Y1[0,i])+extraction_aperture, i])\n",
    "    #print(i, int(Y1[0,i])-extraction_aperture, ' :', int(Y1[0,i])+extraction_aperture)\n",
    "    #print(i, wave_um[i])\n",
    "\n",
    "#print(wave_um)\n",
    "\n",
    "\n",
    "print(np.nanmin(wave_um))\n",
    "print( np.where(wave_um == np.nanmin(wave_um))[0] )\n",
    "\n",
    "i = 20\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.set_title('NIRSpec BOTS '+process_detector.upper()+', integration ' + str(i+1), fontsize=15)\n",
    "ax.errorbar(wave_um[x11:x22], spectra['order1']['flux'][i,x11:x22], yerr=spectra['order1']['flux_errors'][i,x11:x22], color='cornflowerblue')\n",
    "\n",
    "# add minor ticks\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "#ax.set_xlim(np.min(spectra['order1']['x']), np.max(spectra['order1']['x']))\n",
    "\n",
    "ax.set_xlabel('Wavelength, um', fontsize=15)\n",
    "ax.set_ylabel('Total flux in aperture, Counts/s', fontsize=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb033f-c9c6-48a8-b57d-cc597c7a8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an array for all 1D spectra\n",
    "all_spec_1D = np.zeros([nintegrations, len(wave_um)])    \n",
    "for i in range(nintegrations):        \n",
    "    all_spec_1D[i,:] = spectra['order1']['flux'][i,:]\n",
    "\n",
    "\n",
    "# get rid of the nans\n",
    "wave_um = wave_um[x11:x22]\n",
    "all_spec_1D = all_spec_1D[:, x11:x22]\n",
    "FWHM1 = FWHM1[:, x11:x22]\n",
    "X1 = X1[:, x11:x22]\n",
    "Y1 = Y1[:, x11:x22]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9d36a-f636-4b71-8b17-61c51d640a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pickle the extracted spectra, so we don't have to run this step every time\n",
    "if (run_mode == 'process'):\n",
    "    \n",
    "    print('\\nSaving ' + process_detector + preprocessed_file_names[6] + '.')\n",
    "    \n",
    "    spectra1D = {\\\n",
    "    'wave_um':wave_um, \\\n",
    "    'all_spec_1D':all_spec_1D\n",
    "    }\n",
    "    \n",
    "    #if process_detector == 'nrs1':\n",
    "    pickle_out = open( spec2_results + process_detector + preprocessed_file_names[6], 'wb')\n",
    "    pickle.dump(spectra1D, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495354a2-9e66-4493-9d94-a2b20d14751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Restore the extracted 1D spectra with wavelength solution\n",
    "if (run_mode == 'load'):\n",
    "    \n",
    "    print('\\nLoading ' + process_detector + preprocessed_file_names[6] + '.')\n",
    "    \n",
    "    pickle_in = open( spec2_results + process_detector + preprocessed_file_names[6], 'rb')\n",
    "    \n",
    "    spectra1D = pickle.load( pickle_in )\n",
    "    pickle_in.close() # close the file\n",
    "    \n",
    "    print('Showing the pickled data:')\n",
    "    \n",
    "    cnt = 0\n",
    "    for item in spectra1D:\n",
    "        print('The data ', cnt, ' is : ', item)\n",
    "        cnt += 1\n",
    "    \n",
    "    wave_um = spectra1D['wave_um']\n",
    "    all_spec_1D = spectra1D['all_spec_1D']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b52e3-a8eb-405e-b980-26600dedd20a",
   "metadata": {},
   "source": [
    "# <font color='white'>-</font>Step 8: Produce white and spectroscopic light curves and fitting data <a class=\"anchor\" id=\"lcs\"></a>\n",
    "\n",
    "With the extracted spectra and wavelength solution, we can now produce the white and spectroscopic light curves, as well as to check for wavelength dependence of the FWHM and spectroscopic light curves by the tilt event.\n",
    "\n",
    "We will first produce the white light curve by summing the flux over the entire wavelength range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17972c-1249-4c68-ba92-35630864ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlc_flux = np.zeros(nintegrations)\n",
    "\n",
    "for i in range(nintegrations):\n",
    "    wlc_flux[i] = np.sum(all_spec_1D[i,:])\n",
    "\n",
    "# Normalize by the median flux of the first twenty points\n",
    "wlc_flux /= np.median(wlc_flux[0:20])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sigma_wlc = np.sqrt(np.nanvar(wlc_flux[2:100]))\n",
    "plt.plot((all_times-np.nanmean(all_times))*24., wlc_flux, color='red', marker=\"o\", markersize=2, label=f'White light curve, (r.m.s.={round(sigma_wlc*1e6, 0)} ppm)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('White Light Curve for ' + process_detector.upper() + f', $\\lambda=${round(wave_um[0], 4)} - {round(wave_um[len(wave_um)-1], 4)} $\\mu$m', fontsize=15)\n",
    "plt.xlabel('Time since mid-exposure, hr', fontsize=15)\n",
    "plt.ylabel('Normalized flux', fontsize=15)\n",
    "#plt.xlim([1, nintegrations])\n",
    "plt.ylim([0.965,1.003])\n",
    "#plt.xlim([-4.2,-0.5])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(np.nanstd(wlc_flux[2:100])*1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b847fc-5f7b-4529-88fd-68cb7cb5c8c3",
   "metadata": {},
   "source": [
    "This light curve is of a very good quality with a scatter of just 162 ppm fir NRS1 in the first 100 data points.\n",
    "\n",
    "Let's plot the FWHM for a few wavelengths and spectroscopic light curves, and see how the tilt event affects them. The function below will produce FWHM for a specific wavelength band from the data. It can also be used to produce chromatic (aka spectroscopic) light curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de863b1-faba-4938-9d0d-1ebd20a761be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_time_series_for_wavelength(ts_2d_array, ts_wave_um, wave_cen_um, wave_radius_um):\n",
    "    # Inputs: \n",
    "    # ts_data - two dimesional array of e.g., spectra, or FWHM, or centroid information for each integration of the time series\n",
    "    # ts_wave_um - associated wavelength array; has the same number of elements as one of the dimensions of ts_data\n",
    "    # wave_cen_um - wavelength at which we want to obtain the time series quantity \n",
    "    # wave_radius_um - wavelength search radius; indexes of wavelengths +/- the radius will be used to search for indeses \n",
    "    \n",
    "    # determine the size of the ts_data \n",
    "    n_integrations = len(ts_2d_array[:,0])\n",
    "    x_size = len(ts_2d_array[0,:])\n",
    "\n",
    "    ts_vector_wave = np.zeros(n_integrations)\n",
    "\n",
    "    # iterate over each integration, identify indexes at the needed wavelengths and compute median\n",
    "    for i in range(n_integrations):\n",
    "        idx1 = np.where((ts_wave_um >= wave_cen_um - wave_radius_um) & (ts_wave_um < wave_cen_um + wave_radius_um) )[0]\n",
    "        if (len(idx1) == 0):\n",
    "            print('No valid indexes in wavelength range!')\n",
    "        else:\n",
    "            #print(idx1)\n",
    "            ts_vector_wave[i] = np.nanmedian(ts_2d_array[i,idx1])\n",
    "    return ts_vector_wave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32648f43-2123-4bb7-be72-5b6765d73e17",
   "metadata": {},
   "source": [
    "First, let's plot the spectroscopic light curves and inspect visually for any signs of discrepancy of the flux with wavelength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432ed4c-fe95-4019-8486-ce2425e5e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define wavelength radius and central wavelengths\n",
    "wave_radius_um = 0.025\n",
    "\n",
    "if (process_detector == 'nrs1'):\n",
    "    wave_center_um = np.array([2.9, 3.1, 3.3, 3.5, 3.7])\n",
    "    wave_colors    = ['#E377C2', '#1F77B4', '#2CA02C', '#FF7F0E', '#D62728']\n",
    "\n",
    "if (process_detector == 'nrs2'):\n",
    "    wave_center_um = np.array([4.0, 4.2, 4.4, 4.6, 4.8])\n",
    "    wave_colors    = ['#E377C2', '#1F77B4', '#2CA02C', '#FF7F0E', '#D62728']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(len(wave_center_um)):\n",
    "    #print(wave_center_um[i])\n",
    "    lc_wav = obtain_time_series_for_wavelength(all_spec_1D, wave_um, wave_center_um[i], wave_radius_um)\n",
    "    \n",
    "    # normalize the light curve\n",
    "    lc_wav /= np.nanmedian(lc_wav[0:100])\n",
    "    \n",
    "    plt.plot((all_times-np.mean(all_times))*24., lc_wav + 0.01*i, color=wave_colors[i], label=f'{str(wave_center_um[i])}$\\mu$m')\n",
    "\n",
    "plt.title('Spectroscopic light curves for ' + process_detector.upper(), fontsize=15)\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Time since mid-exposure, hr', fontsize=15)\n",
    "plt.ylabel('Normalized flux + constant offset', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857faf1-40b5-483d-8054-415ecccf9e28",
   "metadata": {},
   "source": [
    "A visual inspection doesn't show an obvious discrepancy, besides the offset flux after array index 270. Let's renormalize the light curve after that index using the out of transit flux after the egress and check for offsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c0f98-a306-4a45-8283-c1049c5ffb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define wavelength radius and central wavelengths\n",
    "wave_radius_um = 0.025\n",
    "\n",
    "idx_tilt = 270\n",
    "\n",
    "if (process_detector == 'nrs1'):\n",
    "    wave_center_um = np.array([2.9, 3.1, 3.3, 3.5, 3.7])\n",
    "    wave_colors    = ['#E377C2', '#1F77B4', '#2CA02C', '#FF7F0E', '#D62728']\n",
    "\n",
    "if (process_detector == 'nrs2'):\n",
    "    wave_center_um = np.array([4.0, 4.2, 4.4, 4.6, 4.8])\n",
    "    wave_colors    = ['#E377C2', '#1F77B4', '#2CA02C', '#FF7F0E', '#D62728']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(len(wave_center_um)):\n",
    "    #print(wave_center_um[i])\n",
    "    lc_wav = obtain_time_series_for_wavelength(all_spec_1D, wave_um, wave_center_um[i], wave_radius_um)\n",
    "    \n",
    "    # normalize the light curve\n",
    "    lc_wav /= np.nanmedian(lc_wav[0:100])\n",
    "\n",
    "    # Correct for tilt event\n",
    "    lc_wav[idx_tilt:] /= np.nanmedian(lc_wav[330:440])\n",
    "    \n",
    "    plt.plot((all_times-np.mean(all_times))*24., lc_wav + 0.01*i, color=wave_colors[i], label=f'{str(wave_center_um[i])}$\\mu$m')\n",
    "\n",
    "plt.title('Spectroscopic light curves for ' + process_detector.upper() + ' renormalized after tilt event', fontsize=15)\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Time since mid-exposure, hr', fontsize=15)\n",
    "plt.ylabel('Normalized flux + constant offset', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e63213-0660-43b2-aceb-167c115aeea1",
   "metadata": {},
   "source": [
    "Let's also plot differential light curves with respect to the light curve of the shortest wavelength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef201285-9ae7-441a-be2b-e0b6c25f77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define wavelength radius and central wavelengths\n",
    "wave_radius_um = 0.025\n",
    "\n",
    "if (process_detector == 'nrs1'):\n",
    "    wave_center_um = np.array([2.9, 3.1, 3.3, 3.5, 3.7])\n",
    "    wave_colors    = ['#E377C2', '#1F77B4', '#2CA02C', '#FF7F0E', '#D62728']\n",
    "\n",
    "if (process_detector == 'nrs2'):\n",
    "    wave_center_um = np.array([4.0, 4.2, 4.4, 4.6, 4.8])\n",
    "    wave_colors    = ['#E377C2', '#1F77B4', '#2CA02C', '#FF7F0E', '#D62728']\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for i in range(len(wave_center_um)):\n",
    "    #print(wave_center_um[i])\n",
    "    lc_wav = obtain_time_series_for_wavelength(all_spec_1D, wave_um, wave_center_um[i], wave_radius_um)\n",
    "    \n",
    "    # normalize the light curve\n",
    "    lc_wav /= np.nanmedian(lc_wav[0:100])\n",
    "\n",
    "    if i == 0:\n",
    "        ref_lc = lc_wav\n",
    "    \n",
    "    plt.plot((all_times-np.mean(all_times))*24., lc_wav - ref_lc - 0.015*i, color=wave_colors[i], label=f'{str(wave_center_um[i])}$\\mu$m')\n",
    "    plt.plot((all_times-np.mean(all_times))*24., np.median(lc_wav - ref_lc - 0.015*i)+0.*all_times, color=wave_colors[i], linestyle='dashed')\n",
    "    \n",
    "\n",
    "\n",
    "plt.title('Spectroscopic Differential light curves for ' + process_detector.upper(), fontsize=15)\n",
    "plt.legend(loc='lower left', mode = 'expand', ncol = len(wave_center_um))\n",
    "plt.xlabel('Time since mid-exposure, hr', fontsize=15)\n",
    "plt.ylabel(r'Flux-Flux$_{2.9\\,\\mu m}$ + constant offset', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbd8ef-ebc7-41f6-bf90-172886a7cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define wavelength radius and central wavelengths\n",
    "wave_radius_um = 0.025\n",
    "\n",
    "if (process_detector == 'nrs1'):\n",
    "    wave_center_um = np.array([2.9, 3.1, 3.3, 3.5, 3.7])\n",
    "    wave_colors    = ['#E377C2', '#1F77B4', '#2CA02C', '#FF7F0E', '#D62728']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(len(wave_center_um)):\n",
    "    #print(wave_center_um[i])\n",
    "    fwhm_wav = obtain_time_series_for_wavelength(FWHM1, wave_um, wave_center_um[i], wave_radius_um)\n",
    "    \n",
    "    # normalize the FWHM\n",
    "    fwhm_wav /= np.nanmedian(fwhm_wav[0:100])\n",
    "    \n",
    "    plt.plot((all_times-np.mean(all_times))*24., fwhm_wav, color=wave_colors[i], label=f'{str(wave_center_um[i])}$\\mu$m')\n",
    "\n",
    "\n",
    "plt.title('Spectroscopic FWHM for ' + process_detector.upper(), fontsize=15)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Time since mid-exposure, hr', fontsize=15)\n",
    "plt.ylabel('median FWHM (pixel)', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58dc9fc-5863-4879-b8e6-4bb16e178574",
   "metadata": {},
   "source": [
    "First, the FWHM with wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb17200-b401-46f9-b7f4-8836fbad07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, plot chromatic light curves\n",
    "\n",
    "lc_map = np.copy(all_spec_1D)\n",
    "# \n",
    "spec_xlen = len(lc_map[0,:])\n",
    "\n",
    "\n",
    "correct_tilt_event = 'yes'\n",
    "\n",
    "for j in range(spec_xlen):\n",
    "    # Normalize with each spectral line (removes throughput)\n",
    "    lc_map[:,j] /= np.nanmean(lc_map[0:170,j])\n",
    "    if correct_tilt_event == 'yes':\n",
    "        # Correct for tilt event\n",
    "        lc_map[270:,j] /= np.nanmean(lc_map[330:460,j])\n",
    "\n",
    "\n",
    "mfactor = 1.6\n",
    "plt.figure(figsize=(5*mfactor,8*mfactor))\n",
    "plt.subplot(211)\n",
    "plt.title('Spectroscopic light curves for ' + process_detector.upper(), fontsize=15)\n",
    "im1 = plt.imshow(lc_map, \n",
    "                 interpolation='bilinear', \n",
    "                 aspect='auto', \n",
    "                 cmap='inferno_r', \n",
    "                 origin='lower', clim=(0.977, 1.005 ))\n",
    "\n",
    "#plt.xlabel(r'Wavelength, $\\mu$m', fontsize=15)\n",
    "plt.xlabel(r'x-column, pixel', fontsize=15)\n",
    "plt.ylabel('Integration ', fontsize=15)\n",
    "\n",
    "# Suppress axis\n",
    "#plt.axis('off')\n",
    "\n",
    "#plt.axes()\n",
    "#secax = axes().secondary_yaxis('right', functions=(RPRS_TO_H, H_TO_RPRS))\n",
    "#secax.set_ylabel('Pressure scale height')\n",
    "\n",
    "cb1 = plt.colorbar(label=r'Normalized Flux')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee084341-1aa5-4a6b-831d-7682620a659a",
   "metadata": {},
   "source": [
    "Save the light curves along with all fitting products to be ready for fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b27e3c-0d30-481c-8328-d57f83a24cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nSaving ' + process_detector + preprocessed_file_names[7] + '.')\n",
    "\n",
    "light_curves = {\\\n",
    "'all_spec_1D':all_spec_1D, \\\n",
    "'wave_um':wave_um, \\\n",
    "'all_times':all_times, \\\n",
    "'wlc_flux':wlc_flux, \\\n",
    "'lc_map':lc_map, \\\n",
    "'median_fwhm':median_fwhm, \\\n",
    "'median_yshift':median_yshift, \\\n",
    "'X1':X1, \\\n",
    "'Y1':Y1, \\\n",
    "'FWHM1':FWHM1 \\\n",
    "}\n",
    "\n",
    "#if process_detector == 'nrs1':\n",
    "pickle_out = open( spec2_results + process_detector + preprocessed_file_names[7], 'wb')\n",
    "pickle.dump(light_curves, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee41e9-1ff2-407c-a11a-94ff1e167b10",
   "metadata": {},
   "source": [
    "### <font color='white'>-</font>5. Concluding remarks<a class=\"anchor\" id=\"remarks\"></a>\n",
    "\n",
    "In this notebook, we demonstrated how to obtain light curves and fitting data products starting from the rateints files, and using a step from the JWST pipeline and our custom steps. The saved data producs can now be provided to light curve fitting codes for measurements of the physical properties of the exoplanet and obtaining a transmission spectrum. It should be pointed out that the analyses performed here are only a subset of the possible analyses one can perform, and are in no way the final word on _how_ JWST data _should_ be analyzed. This will be solidified more and more as data comes and best practices are established in the current and future cycles.\n",
    "\n",
    "In conclusion, I would like to express my gratitude to the entire JWST team that has supported the creation of this notebook through discussions and testing, which have improved the notebook. In particular, special thanks to the Time-Series Observations Working Group at STScI, including Néstor Espinoza, Leonardo Ubeda, Sarah Kendrew, Elena Manjavacas, Brian Brooks, Mike Reagan, Loïc Albert, Everett Schlawin, Stephan Birkmann among others. To the NIRCam IDT team for multiple fruitful discussions, including Everett Schlawin, Thomas Beatty, Tom Greene and Jarron Leisenring. To the ERS Transiting Exoplanet team who have provided several venues for discussion and community input. To the several JWST team members, including behind the pipeline and the mission itself, including and in no particular order Bryan Hilbert, Armin Rest, Anton Koekemoer, Alicia Canipe, Melanie Clarke, James Muzerolle, Kayli Glidic, Jeff Valenti and Karl Gordon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d9521-ed82-401c-ba7d-721748731e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b52043-bc99-440f-b96a-6753f1f6fc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de0230-b37b-4a36-8d31-10020ff232e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "masterclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
